{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "1. Importing Necessary Libraries\nTo start, let's print out the version numbers of all the libraries we will be using in this project. This serves two purposes - it ensures we have installed the libraries correctly and ensures that this tutorial will be reproducible.\nIn [1]:\nimport sys\nimport numpy\nimport pandas\nimport matplotlib\nimport seaborn\nimport scipy\n\nprint('Python: {}'.format(sys.version))\nprint('Numpy: {}'.format(numpy.__version__))\nprint('Pandas: {}'.format(pandas.__version__))\nprint('Matplotlib: {}'.format(matplotlib.__version__))\nprint('Seaborn: {}'.format(seaborn.__version__))\nprint('Scipy: {}'.format(scipy.__version__))\nPython: 2.7.13 |Continuum Analytics, Inc.| (default, May 11 2017, 13:17:26) [MSC v.1500 64 bit (AMD64)]\nNumpy: 1.14.0\nPandas: 0.21.0\nMatplotlib: 2.1.0\nSeaborn: 0.8.1\nScipy: 1.0.0\nIn [2]:\n# import the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n2. The Data Set\nIn the following cells, we will import our dataset from a .csv file as a Pandas DataFrame. Furthermore, we will begin exploring the dataset to gain an understanding of the type, quantity, and distribution of data in our dataset. For this purpose, we will use Pandas' built-in describe feature, as well as parameter histograms and a correlation matrix.\nIn [3]:\n# Load the dataset from the csv file using pandas\ndata = pd.read_csv('creditcard.csv')\nIn [4]:\n# Start exploring the dataset\nprint(data.columns)\nIndex([u'Time', u'V1', u'V2', u'V3', u'V4', u'V5', u'V6', u'V7', u'V8', u'V9',\n       u'V10', u'V11', u'V12', u'V13', u'V14', u'V15', u'V16', u'V17', u'V18',\n       u'V19', u'V20', u'V21', u'V22', u'V23', u'V24', u'V25', u'V26', u'V27',\n       u'V28', u'Amount', u'Class'],\n      dtype='object')\nIn [5]:\n# Print the shape of the data\ndata = data.sample(frac=0.1, random_state = 1)\nprint(data.shape)\nprint(data.describe())\n\n# V1 - V28 are the results of a PCA Dimensionality reduction to protect user identities and sensitive features\n(28481, 31)\n                Time            V1            V2            V3            V4  \\\ncount   28481.000000  28481.000000  28481.000000  28481.000000  28481.000000   \nmean    94705.035216     -0.001143     -0.018290      0.000795      0.000350   \nstd     47584.727034      1.994661      1.709050      1.522313      1.420003   \nmin         0.000000    -40.470142    -63.344698    -31.813586     -5.266509   \n25%     53924.000000     -0.908809     -0.610322     -0.892884     -0.847370   \n50%     84551.000000      0.031139      0.051775      0.178943     -0.017692   \n75%    139392.000000      1.320048      0.792685      1.035197      0.737312   \nmax    172784.000000      2.411499     17.418649      4.069865     16.715537   \n\n                 V5            V6            V7            V8            V9  \\\ncount  28481.000000  28481.000000  28481.000000  28481.000000  28481.000000   \nmean      -0.015666      0.003634     -0.008523     -0.003040      0.014536   \nstd        1.395552      1.334985      1.237249      1.204102      1.098006   \nmin      -42.147898    -19.996349    -22.291962    -33.785407     -8.739670   \n25%       -0.703986     -0.765807     -0.562033     -0.208445     -0.632488   \n50%       -0.068037     -0.269071      0.028378      0.024696     -0.037100   \n75%        0.603574      0.398839      0.559428      0.326057      0.621093   \nmax       28.762671     22.529298     36.677268     19.587773      8.141560   \n\n           ...                V21           V22           V23           V24  \\\ncount      ...       28481.000000  28481.000000  28481.000000  28481.000000   \nmean       ...           0.004740      0.006719     -0.000494     -0.002626   \nstd        ...           0.744743      0.728209      0.645945      0.603968   \nmin        ...         -16.640785    -10.933144    -30.269720     -2.752263   \n25%        ...          -0.224842     -0.535877     -0.163047     -0.360582   \n50%        ...          -0.029075      0.014337     -0.012678      0.038383   \n75%        ...           0.189068      0.533936      0.148065      0.434851   \nmax        ...          22.588989      6.090514     15.626067      3.944520   \n\n                V25           V26           V27           V28        Amount  \\\ncount  28481.000000  28481.000000  28481.000000  28481.000000  28481.000000   \nmean      -0.000917      0.004762     -0.001689     -0.004154     89.957884   \nstd        0.520679      0.488171      0.418304      0.321646    270.894630   \nmin       -7.025783     -2.534330     -8.260909     -9.617915      0.000000   \n25%       -0.319611     -0.328476     -0.071712     -0.053379      5.980000   \n50%        0.015231     -0.049750      0.000914      0.010753     22.350000   \n75%        0.351466      0.253580      0.090329      0.076267     78.930000   \nmax        5.541598      3.118588     11.135740     15.373170  19656.530000   \n\n              Class  \ncount  28481.000000  \nmean       0.001720  \nstd        0.041443  \nmin        0.000000  \n25%        0.000000  \n50%        0.000000  \n75%        0.000000  \nmax        1.000000  \n\n[8 rows x 31 columns]\nIn [6]:\n# Plot histograms of each parameter \ndata.hist(figsize = (20, 20))\nplt.show()\n\nIn [7]:\n# Determine number of fraud cases in dataset\n\nFraud = data[data['Class'] == 1]\nValid = data[data['Class'] == 0]\n\noutlier_fraction = len(Fraud)/float(len(Valid))\nprint(outlier_fraction)\n\nprint('Fraud Cases: {}'.format(len(data[data['Class'] == 1])))\nprint('Valid Transactions: {}'.format(len(data[data['Class'] == 0])))\n0.00172341024198\nFraud Cases: 49\nValid Transactions: 28432\nIn [8]:\n# Correlation matrix\ncorrmat = data.corr()\nfig = plt.figure(figsize = (12, 9))\n\nsns.heatmap(corrmat, vmax = .8, square = True)\nplt.show()\n\nIn [9]:\n# Get all the columns from the dataFrame\ncolumns = data.columns.tolist()\n\n# Filter the columns to remove data we do not want\ncolumns = [c for c in columns if c not in [\"Class\"]]\n\n# Store the variable we'll be predicting on\ntarget = \"Class\"\n\nX = data[columns]\nY = data[target]\n\n# Print shapes\nprint(X.shape)\nprint(Y.shape)\n(28481, 30)\n(28481L,)\n3. Unsupervised Outlier Detection\nNow that we have processed our data, we can begin deploying our machine learning algorithms. We will use the following techniques:\nLocal Outlier Factor (LOF)\nThe anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood.\nIsolation Forest Algorithm\nThe IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\nSince recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\nThis path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\nRandom partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.\nIn [11]:\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# define random states\nstate = 1\n\n# define outlier detection tools to be compared\nclassifiers = {\n    \"Isolation Forest\": IsolationForest(max_samples=len(X),\n                                        contamination=outlier_fraction,\n                                        random_state=state),\n    \"Local Outlier Factor\": LocalOutlierFactor(\n        n_neighbors=20,\n        contamination=outlier_fraction)}\nIn [15]:\n# Fit the model\nplt.figure(figsize=(9, 7))\nn_outliers = len(Fraud)\n\n\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    \n    # fit the data and tag outliers\n    if clf_name == \"Local Outlier Factor\":\n        y_pred = clf.fit_predict(X)\n        scores_pred = clf.negative_outlier_factor_\n    else:\n        clf.fit(X)\n        scores_pred = clf.decision_function(X)\n        y_pred = clf.predict(X)\n    \n    # Reshape the prediction values to 0 for valid, 1 for fraud. \n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n    \n    n_errors = (y_pred != Y).sum()\n    \n    # Run classification metrics\n    print('{}: {}'.format(clf_name, n_errors))\n    print(accuracy_score(Y, y_pred))\n    print(classification_report(Y, y_pred))\nLocal Outlier Factor: 97\n0.9965942207085425\n             precision    recall  f1-score   support\n\n          0       1.00      1.00      1.00     28432\n          1       0.02      0.02      0.02        49\n\navg / total       1.00      1.00      1.00     28481\n\nIsolation Forest: 71\n0.99750711000316\n             precision    recall  f1-score   support\n\n          0       1.00      1.00      1.00     28432\n          1       0.28      0.29      0.28        49\n\navg / total       1.00      1.00      1.00     28481",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}